from urllib.request import urlopen
from urllib.error import HTTPError
from html import unescape
from pathlib import Path
import base64

FILE_CACHE_DIR = "wiki_cache"

class Internet:
    DISALLOWED = [":", "#", "/", "?"]
    """
    This class represents the Internet. Feel free to look,
    but do NOT edit this file. Also, do NOT access the
    Internet in any way except through this class,
    or you will see unexpected results.

    get_page(page) will return the HTML code from a given page.
    page should be formatted as "/wiki/Something", which will
    give the HTML for the page https://en.wikipedia.org/wiki/Something.

    get_random() is a function you can use if you desire. It will return the HTML
    of a random page on Wikipedia.

    at_time is an optional parameter that will access Wikipedia as it was on `at_time`, 
    where `at_time` is a string that looks like "YYYYMMDDHHMMSS". For example,
    to access Wikipedia pages as they were on April 1, 2010, you would set
    `at_time` to "20100401000000". Set it to None to view current Wikipedia.

    Note that setting at_time != None will double the runtime of your Wikiracer,
    because getting the revision history takes an extra internet request. However,
    this extra request will not be added to self.requests, though it will take time.



    Usage of Internet:
    internet = Internet()
    html = internet.get_page("/wiki/Computer_science")
    print(html)

    """

    def __init__(self, at_time=None):
        self.requests = []
        self.at_time = at_time

    def get_page(self, page):
        if page[:6] != "/wiki/":
            raise ValueError(f"Links must start with /wiki/. {page} is not valid.")
        if any(i in page[6:] for i in Internet.DISALLOWED):
            raise ValueError(f"Link cannot contain disallowed character. {page} is not valid.")
        self.requests.append(page)
        return self.__get_page_internal(page)

    # You may find this useful in your wikiracer implementation.
    def get_random(self):
        self.requests.append("Random")
        return Internet.__readurl("https://en.wikipedia.org/wiki/Special:Random")

    def __get_page_internal(self, page):
        # First see if we have it in the local cache, to reduce the number of spam requests to Wikipedia
        file_cache_dir_path = Path(FILE_CACHE_DIR)
        if not file_cache_dir_path.is_dir():
            file_cache_dir_path.mkdir()

        # Convert page to a filesystem safe name
        title = page[6:]
        fs_name_unsafe = title + ":" + str(self.at_time)
        safe_name = base64.urlsafe_b64encode(fs_name_unsafe.encode("utf-8")).decode("utf-8")

        local_path = file_cache_dir_path / safe_name
        if local_path.is_file():
            return local_path.read_text(encoding="utf-8")

        url = self.__get_url_at_time_internal(page)
        html = Internet.__readurl(f"https://en.wikipedia.org{url}")

        # write to file cache
        local_path.write_text(html, encoding="utf-8")

        return html

    def __get_url_at_time_internal(self, page):
        if self.at_time is None:
            return page
        title = page[6:]
        revision_page = f"/w/index.php?title={title}&action=history&offset={self.at_time}"
        revision_html = Internet.__readurl(f"https://en.wikipedia.org{revision_page}")
        start_of_url = revision_html.find(f"/w/index.php?title={title}&amp;oldid=")
        end_of_url = revision_html.find('"', start_of_url)
        return unescape(revision_html[start_of_url : end_of_url])

    def __readurl(url):
        html = "ERROR"
        try:
            with urlopen(url) as response:
                html = response.read().decode("utf-8")
        except HTTPError:
            pass
        return html